
@misc{wang_survey_2023,
	title = {Survey on {Factuality} in {Large} {Language} {Models}: {Knowledge}, {Retrieval} and {Domain}-{Specificity}},
	shorttitle = {Survey on {Factuality} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.07521},
	doi = {10.48550/arXiv.2310.07521},
	abstract = {This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
	month = dec,
	year = {2023},
	note = {arXiv:2310.07521 [cs]},
	keywords = {type:concepts},
	annote = {Comment: 62 pages; 300+ references},
	file = {PDF:/Users/lamhu/Zotero/storage/BA4WZEUQ/Wang et al. - 2023 - Survey on Factuality in Large Language Models Knowledge, Retrieval and Domain-Specificity.pdf:application/pdf},
}

@misc{zhang_sirens_2023,
	title = {Siren's {Song} in the {AI} {Ocean}: {A} {Survey} on {Hallucination} in {Large} {Language} {Models}},
	shorttitle = {Siren's {Song} in the {AI} {Ocean}},
	url = {http://arxiv.org/abs/2309.01219},
	doi = {10.48550/arXiv.2309.01219},
	abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	month = sep,
	year = {2023},
	note = {arXiv:2309.01219 [cs]},
	keywords = {type:concepts},
	annote = {Comment: work in progress; 32 pages},
	file = {PDF:/Users/lamhu/Zotero/storage/GNKT6DLJ/Zhang et al. - 2023 - Siren's Song in the AI Ocean A Survey on Hallucination in Large Language Models.pdf:application/pdf},
}

@misc{thorne_fever_2018,
	title = {{FEVER}: a large-scale dataset for {Fact} {Extraction} and {VERification}},
	shorttitle = {{FEVER}},
	url = {http://arxiv.org/abs/1803.05355},
	doi = {10.48550/arXiv.1803.05355},
	abstract = {In this paper we introduce a new publicly available dataset for veriﬁcation against textual sources, FEVER: Fact Extraction and VERiﬁcation. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently veriﬁed without knowledge of the sentence they were derived from. The claims are classiﬁed as SUPPORTED, REFUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the ﬁrst two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87\%, while if we ignore the evidence we achieve 50.91\%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim veriﬁcation against textual sources.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
	month = dec,
	year = {2018},
	note = {arXiv:1803.05355 [cs]},
	keywords = {type:benchmarks},
	annote = {Comment: Updated version of NAACL2018 paper. Data is released on http://fever.ai},
	file = {PDF:/Users/lamhu/Zotero/storage/BUBYRXMR/Thorne et al. - 2018 - FEVER a large-scale dataset for Fact Extraction and VERification.pdf:application/pdf},
}

@misc{schuster_get_2021,
	title = {Get {Your} {Vitamin} {C}! {Robust} {Fact} {Verification} with {Contrastive} {Evidence}},
	url = {http://arxiv.org/abs/2103.08541},
	doi = {10.48550/arXiv.2103.08541},
	abstract = {Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness -- improving accuracy by 10\% on adversarial fact verification and 6\% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Schuster, Tal and Fisch, Adam and Barzilay, Regina},
	month = mar,
	year = {2021},
	note = {arXiv:2103.08541 [cs]},
	keywords = {type:benchmarks},
	annote = {Comment: NAACL 2021},
	file = {PDF:/Users/lamhu/Zotero/storage/8YC8CWRZ/Schuster et al. - 2021 - Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence.pdf:application/pdf},
}

@misc{wei_long-form_2024,
	title = {Long-form factuality in large language models},
	url = {http://arxiv.org/abs/2403.18802},
	doi = {10.48550/arXiv.2403.18802},
	abstract = {Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for longform factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall).},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Huang, Jie and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and Le, Quoc V.},
	month = nov,
	year = {2024},
	note = {arXiv:2403.18802 [cs]},
	keywords = {type:methods},
	annote = {Comment: NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at https://github.com/google-deepmind/long-form-factuality},
	file = {PDF:/Users/lamhu/Zotero/storage/YH2FZCWC/Wei et al. - 2024 - Long-form factuality in large language models.pdf:application/pdf},
}

@misc{wang_asking_2020,
	title = {Asking and {Answering} {Questions} to {Evaluate} the {Factual} {Consistency} of {Summaries}},
	url = {http://arxiv.org/abs/2004.04228},
	doi = {10.48550/arXiv.2004.04228},
	abstract = {Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called QAGS1 that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Wang, Alex and Cho, Kyunghyun and Lewis, Mike},
	month = apr,
	year = {2020},
	note = {arXiv:2004.04228 [cs]},
	keywords = {type:methods},
	annote = {Comment: ACL 2020},
	file = {PDF:/Users/lamhu/Zotero/storage/IHHFVZCS/Wang et al. - 2020 - Asking and Answering Questions to Evaluate the Factual Consistency of Summaries.pdf:application/pdf},
}

@misc{maynez_faithfulness_2020,
	title = {On {Faithfulness} and {Factuality} in {Abstractive} {Summarization}},
	url = {http://arxiv.org/abs/2005.00661},
	doi = {10.48550/arXiv.2005.00661},
	abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
	month = may,
	year = {2020},
	note = {arXiv:2005.00661 [cs]},
	keywords = {type:concepts},
	annote = {Comment: ACL 2020, 14 pages},
	file = {PDF:/Users/lamhu/Zotero/storage/7I6T2WDK/Maynez et al. - 2020 - On Faithfulness and Factuality in Abstractive Summarization.pdf:application/pdf},
}

@misc{laban_summac_2021,
	title = {{SummaC}: {Re}-{Visiting} {NLI}-based {Models} for {Inconsistency} {Detection} in {Summarization}},
	shorttitle = {{SummaC}},
	url = {http://arxiv.org/abs/2111.09525},
	doi = {10.48550/arXiv.2111.09525},
	abstract = {In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, ﬁnding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SUMMACConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. On our newly introduced benchmark called SUMMAC (Summary Consistency) consisting of six large inconsistency detection datasets, SUMMACConv obtains stateof-the-art results with a balanced accuracy of 74.4\%, a 5\% point improvement compared to prior work.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Laban, Philippe and Schnabel, Tobias and Bennett, Paul N. and Hearst, Marti A.},
	month = nov,
	year = {2021},
	note = {arXiv:2111.09525 [cs]},
	keywords = {type:methods},
	annote = {Comment: TACL pre-MIT Press publication version; 11 pages, 2 figures, 5 tables},
	file = {PDF:/Users/lamhu/Zotero/storage/JFXE9UYQ/Laban et al. - 2021 - SummaC Re-Visiting NLI-based Models for Inconsistency Detection in Summarization.pdf:application/pdf},
}

@misc{pagnoni_understanding_2021,
	title = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}: {A} {Benchmark} for {Factuality} {Metrics}},
	shorttitle = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}},
	url = {http://arxiv.org/abs/2104.13346},
	doi = {10.48550/arXiv.2104.13346},
	abstract = {Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations, we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgment as well as their specific strengths and weaknesses.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Pagnoni, Artidoro and Balachandran, Vidhisha and Tsvetkov, Yulia},
	month = jul,
	year = {2021},
	note = {arXiv:2104.13346 [cs]},
	keywords = {type:benchmarks},
	annote = {Comment: Accepted at NAACL 2021. Second version fixes bug with BERTScore results},
	file = {PDF:/Users/lamhu/Zotero/storage/7ZWIS873/Pagnoni et al. - 2021 - Understanding Factuality in Abstractive Summarization with FRANK A Benchmark for Factuality Metrics.pdf:application/pdf},
}

@misc{agarwal_zero-shot_2024,
	title = {Zero-shot {Factual} {Consistency} {Evaluation} {Across} {Domains}},
	url = {http://arxiv.org/abs/2408.04114},
	doi = {10.48550/arXiv.2408.04114},
	abstract = {This work addresses the challenge of factual consistency in text generation systems. We unify the tasks of Natural Language Inference, Summarization Evaluation, Factuality Verification and Factual Consistency Evaluation to train models capable of evaluating the factual consistency of source-target pairs across diverse domains. We rigorously evaluate these against eight baselines on a comprehensive benchmark suite comprising 22 datasets that span various tasks, domains, and document lengths. Results demonstrate that our method achieves state-of-the-art performance on this heterogeneous benchmark while addressing efficiency concerns and attaining cross-domain generalization.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Agarwal, Raunak},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04114 [cs]},
	keywords = {type:methods},
	file = {PDF:/Users/lamhu/Zotero/storage/47C5SH27/Agarwal - 2024 - Zero-shot Factual Consistency Evaluation Across Domains.pdf:application/pdf},
}
